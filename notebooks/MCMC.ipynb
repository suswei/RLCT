{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EglWNxVn9whz"
   },
   "source": [
    "# Hamiltonian Monte Carlo estimates of RLCT\n",
    "\n",
    "We train a small neural network to \"convergence\" at a weight vector `w_0` and then initialise Hamiltonian Monte Carlo (HMC) estimation of the posterior with a chain starting at `w_0`. We run this HMC with some step size adaptation, and wait for it to \"plateau\" in the sense that the acceptance rate becomes high (which we interpret as the chain finding something like a local minima, from which it is difficult to escape at the final step size). Let `w_1` be this weight vector at which the HMC chain is temporarily stationary. We then restart HMC at a smaller step size, but with a modified log probability distribution which prevents the chain from exiting an epsilon ball around `w_1`.\n",
    "\n",
    "Finally we sample from this probability distribution on the epsilon ball around `w_1` to generate an estimate of the local Real Log Canonical Threshold (RLCT) of the neural network around `w_1`.\n",
    "\n",
    "This code is heavily based on notebooks found here:\n",
    "https://colab.research.google.com/drive/1bWQcuR5gaBPpow6ARKnPPL-dtf2EvTae#scrollTo=zNhVpzE95IGp\n",
    "https://github.com/tensorflow/probability/issues/292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlZv2O4k9fOv"
   },
   "outputs": [],
   "source": [
    "# Installation\n",
    "#%%bash\n",
    "#cd /tmp\n",
    "#wget https://github.com/tensorflow/probability/files/3103693/data.zip\n",
    "#unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6McV7EO93FU"
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mSgz2rFl6YI"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k39XXUBSl3VJ"
   },
   "outputs": [],
   "source": [
    "def dense(X, W, b, activation):\n",
    "    return activation(tf.matmul(X, W) + b)\n",
    "\n",
    "def build_network(weights_list, biases_list, activation=tf.nn.relu):\n",
    "    def model(X):\n",
    "        net = X\n",
    "        for (weights, biases) in zip(weights_list[:-1], biases_list[:-1]):\n",
    "            net = dense(net, weights, biases, activation)\n",
    "        # final linear layer\n",
    "        net = tf.matmul(net, weights_list[-1]) + biases_list[-1]\n",
    "        preds = net[:, 0]\n",
    "        std_devs = tf.exp(-net[:, 1])\n",
    "        # preds and std_devs each have size N = X.shape(0) (the number of data samples)\n",
    "        # and are the model's predictions and (log-sqrt of) learned loss attenuations, resp.\n",
    "        return tfd.Normal(loc=preds, scale=std_devs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_initial_state(weight_prior, bias_prior, num_features, layers=None):\n",
    "    \"\"\"generate starting point for creating Markov chain\n",
    "        of weights and biases for fully connected NN\n",
    "    Keyword Arguments:\n",
    "        layers {tuple} -- number of nodes in each layer of the network\n",
    "    Returns:\n",
    "        list -- architecture of FCNN with weigths and bias tensors for each layer\n",
    "    \"\"\"\n",
    "    # make sure the last layer has two nodes, so that output can be split into\n",
    "    # predictive mean and learned loss attenuation (see https://arxiv.org/abs/1703.04977)\n",
    "    # which the network learns individually\n",
    "    if layers is not None:\n",
    "        assert layers[-1] == 2\n",
    "    if layers is None:\n",
    "        layers = (\n",
    "            num_features,\n",
    "            num_features // 2,\n",
    "            num_features // 5,\n",
    "            num_features // 10,\n",
    "            2,\n",
    "        )\n",
    "    else:\n",
    "        layers.insert(0, num_features)\n",
    "\n",
    "    architecture = []\n",
    "    for idx in range(len(layers) - 1):\n",
    "        weigths = weight_prior.sample((layers[idx], layers[idx + 1]))\n",
    "        biases = bias_prior.sample((layers[idx + 1]))\n",
    "        # weigths = tf.zeros((layers[idx], layers[idx + 1]))\n",
    "        # biases = tf.zeros((layers[idx + 1]))\n",
    "        architecture.extend((weigths, biases))\n",
    "    return architecture\n",
    "\n",
    "\n",
    "def bnn_joint_log_prob_fn(weight_prior, bias_prior, X, y, *args):\n",
    "    weights_list = args[::2]\n",
    "    biases_list = args[1::2]\n",
    "\n",
    "    # prior log-prob\n",
    "    lp = sum(\n",
    "        [tf.reduce_sum(weight_prior.log_prob(weights)) for weights in weights_list]\n",
    "    )\n",
    "    lp += sum([tf.reduce_sum(bias_prior.log_prob(bias)) for bias in biases_list])\n",
    "\n",
    "    # likelihood of predicted labels\n",
    "    network = build_network(weights_list, biases_list)\n",
    "    labels_dist = network(X.astype(\"float32\"))\n",
    "    lp += tf.reduce_sum(labels_dist.log_prob(y))\n",
    "    return lp\n",
    "\n",
    "\n",
    "def trace_fn(current_state, results, summary_freq=100):\n",
    "    #step = results.step\n",
    "    #with tf.summary.record_if(tf.equal(step % summary_freq, 0)):\n",
    "    #    for idx, tensor in enumerate(current_state, 1):\n",
    "    #        count = str(math.ceil(idx / 2))\n",
    "    #        name = \"weights_\" if idx % 2 == 0 else \"biases_\" + count\n",
    "    #        tf.summary.histogram(name, tensor, step=tf.cast(step, tf.int64))\n",
    "    return results\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def graph_hmc(*args, **kwargs):\n",
    "    \"\"\"Compile static graph for tfp.mcmc.sample_chain.\n",
    "    Since this is bulk of the computation, using @tf.function here\n",
    "    signifcantly improves performance (empirically about ~5x).\n",
    "    \"\"\"\n",
    "    return tfp.mcmc.sample_chain(*args, **kwargs)\n",
    "\n",
    "\n",
    "def nest_concat(*args):\n",
    "    return tf.nest.map_structure(lambda *parts: tf.concat(parts, axis=0), *args)\n",
    "\n",
    "\n",
    "def run_hmc(\n",
    "    target_log_prob_fn,\n",
    "    step_size=0.01,\n",
    "    num_leapfrog_steps=3,\n",
    "    num_burnin_steps=1000,\n",
    "    num_adaptation_steps=800,\n",
    "    num_results=1000,\n",
    "    num_steps_between_results=0,\n",
    "    current_state=None,\n",
    "    logdir=\"/tmp/data/output/hmc/\",\n",
    "    resume=None,\n",
    "):\n",
    "    \"\"\"Populates a Markov chain by performing `num_results` gradient-informed steps with a\n",
    "    Hamiltonian Monte Carlo transition kernel to produce a Metropolis proposal. Either\n",
    "    that or the previous state is appended to the chain at each step.\n",
    "\n",
    "    Arguments:\n",
    "        target_log_prob_fn {callable} -- Determines the HMC transition kernel\n",
    "        and thereby the stationary distribution that the Markov chain will approximate.\n",
    "        \n",
    "    Returns:\n",
    "        (chain(s), trace, final_kernel_result) -- The Markov chain(s), the trace created by `trace_fn`\n",
    "        and the kernel results of the last step.\n",
    "    \"\"\"\n",
    "    assert (current_state, resume) != (None, None)\n",
    "\n",
    "    # Set up logging.\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = logdir + stamp\n",
    "    summary_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "    kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn, step_size=step_size, num_leapfrog_steps=num_leapfrog_steps\n",
    "    )\n",
    "    kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "        kernel, num_adaptation_steps=num_adaptation_steps\n",
    "    )\n",
    "    #kernel = tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(target_log_prob_fn=target_log_prob_fn, step_size=0.01, volatility_fn = lambda *args: 0.)\n",
    "    if resume is None:\n",
    "        prev_kernel_results = kernel.bootstrap_results(current_state)\n",
    "        step = 0\n",
    "    else:\n",
    "        prev_chain, prev_trace, prev_kernel_results = resume\n",
    "        step = len(prev_chain)\n",
    "        current_state = tf.nest.map_structure(lambda chain: chain[-1], prev_chain)\n",
    "\n",
    "    tf.summary.trace_on(graph=True, profiler=True)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(\n",
    "            name=\"mcmc_sample_trace\", step=step, profiler_outdir=logdir\n",
    "        )\n",
    "        chain, trace, final_kernel_results = graph_hmc(\n",
    "            kernel=kernel,\n",
    "            current_state=current_state,\n",
    "            num_burnin_steps=num_burnin_steps,\n",
    "            num_results=num_burnin_steps + num_results,\n",
    "            previous_kernel_results=prev_kernel_results,\n",
    "            num_steps_between_results=num_steps_between_results,\n",
    "            trace_fn=partial(trace_fn, summary_freq=20),\n",
    "            return_final_kernel_results=True,\n",
    "        )\n",
    "    summary_writer.close()\n",
    "\n",
    "    if resume:\n",
    "        chain = nest_concat(prev_chain, chain)\n",
    "        trace = nest_concat(prev_trace, trace)\n",
    "\n",
    "    return chain, trace, final_kernel_results\n",
    "\n",
    "\n",
    "def get_data(test_size=0.1, random_state=0):\n",
    "    with open(\"/tmp/features.csv\") as file:\n",
    "        features = np.genfromtxt(file, delimiter=\",\")\n",
    "    with open(\"/tmp/labels.csv\") as file:\n",
    "        labels = np.genfromtxt(file, delimiter=\",\")\n",
    "\n",
    "    labels = np.log(labels).reshape(-1, 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "    y_scaler = sklearn.preprocessing.StandardScaler().fit(y_train)\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "\n",
    "    return (X_train, X_test), (y_train, y_test), (X_scaler, y_scaler)\n",
    "\n",
    "# See the discussion of the format of chain in \"HMC from MAP\" below. \n",
    "def plot_curves(chain):\n",
    "  weights_list = chain[::2]\n",
    "  biases_list = chain[1::2]\n",
    "\n",
    "  train_trace = []\n",
    "  test_trace = []\n",
    "  for i in range(len(weights_list[0])):\n",
    "    network = build_network([w[i] for w in weights_list], [b[i] for b in biases_list])(X_train.astype(np.float32))\n",
    "    train_trace.append(-tf.reduce_mean(network.log_prob(y_train[:, 0])).numpy())\n",
    "    network = build_network([w[i] for w in weights_list], [b[i] for b in biases_list])(X_test.astype(np.float32))\n",
    "    test_trace.append(-tf.reduce_mean(network.log_prob(y_test[:, 0])).numpy())\n",
    "  \n",
    "  plt.plot(train_trace, label='train')\n",
    "  plt.plot(test_trace, label='test')\n",
    "  plt.legend(loc='best')\n",
    "    \n",
    "def plot_distances(chain, center):\n",
    "  weights_list = chain[::2]\n",
    "  biases_list = chain[1::2]\n",
    "\n",
    "  center_weights = center[::2]\n",
    "  center_biases = center[1::2]\n",
    "      \n",
    "  distances = []\n",
    "  \n",
    "  # weights_list[0] is of length the number of samples in the Markov chain\n",
    "  for i in range(len(weights_list[0])):\n",
    "    d = 0.0\n",
    "    \n",
    "    for (w,wprime) in zip(weights_list,center_weights):\n",
    "      d += tf.square(tf.norm(w[i] - wprime))\n",
    "    \n",
    "    for (b,bprime) in zip(biases_list,center_biases):\n",
    "      d += tf.square(tf.norm(b[i] - bprime))\n",
    "    \n",
    "    d = tf.sqrt(d)\n",
    "\n",
    "    distances.append(d)\n",
    "  \n",
    "  #print(np.mean(distances))\n",
    "  plt.plot(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka1slITrKjjt"
   },
   "source": [
    "# 2. Train the neural network\n",
    "\n",
    "The final network weights are `map_initial_state`.\n",
    "### 2.1 Setup priors and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3jbs0OK5LIs"
   },
   "outputs": [],
   "source": [
    "# Setup priors and data\n",
    "weight_prior = tfd.Normal(0.0, 0.1)\n",
    "bias_prior = tfd.Normal(0.0, 1.0)\n",
    "(X_train, X_test), (y_train, y_test), scalers = get_data()\n",
    "\n",
    "bnn_joint_log_prob = partial(\n",
    "    bnn_joint_log_prob_fn, weight_prior, bias_prior, X_train, y_train[:, 0]\n",
    ")\n",
    "num_features = X_train.shape[1]\n",
    "initial_state = get_initial_state(weight_prior, bias_prior, num_features)\n",
    "\n",
    "z = 0\n",
    "for s in initial_state:\n",
    "  print(\"State shape\", s.shape)\n",
    "  z += s.shape.num_elements()\n",
    "print(\"Total params\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 20000 # number of SGD steps\n",
    "save_every = 100\n",
    "initial_lr = 1e-5\n",
    "lr = initial_lr\n",
    "num_repeats = 2\n",
    "\n",
    "state_vars = [tf.Variable(s) for s in initial_state]\n",
    "opt = tf.optimizers.SGD(learning_rate=initial_lr)\n",
    "def map_loss():\n",
    "  return -bnn_joint_log_prob(*state_vars)\n",
    "  \n",
    "@tf.function\n",
    "def minimize():\n",
    "  opt.minimize(map_loss, state_vars)\n",
    "  \n",
    "# Traces is a list, one entry per weight/bias, of values\n",
    "# obtained over the course of training\n",
    "traces = [[] for _ in range(len(initial_state))]\n",
    "\n",
    "for r in range(num_repeats):\n",
    "  \n",
    "  for i in range(num_iters):\n",
    "    if i % save_every == 0:\n",
    "      for t, s in zip(traces, state_vars):\n",
    "        t.append(s.numpy())\n",
    "    minimize()\n",
    "    \n",
    "  opt.learning_rate = 1e-2*opt.learning_rate\n",
    "  \n",
    "map_trace = [np.array(t) for t in traces]\n",
    "\n",
    "# map_initial_state contains the same number of entries as initial_state\n",
    "# note that this currently has 8 entries (one per weight matrix and bias)\n",
    "# and each of those entries is a tensor, shapes given above\n",
    "map_initial_state = [tf.constant(t[-1]) for t in map_trace]\n",
    "\n",
    "# Graph training and test performance\n",
    "plot_curves(map_trace)\n",
    "plt.ylim(-1, 2)\n",
    "plt.yticks(np.linspace(-1, 2, 16));\n",
    "\n",
    "d = 0.0\n",
    "for x in map_initial_state:\n",
    "  d += tf.square(tf.norm(x))\n",
    "\n",
    "print(\"Norm of MAP:\")\n",
    "tf.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnJc7DIWKnTN"
   },
   "source": [
    "# 3. Hamiltonian Monte Carlo from MAP\n",
    "\n",
    "We have already trained the network by gradient descent to as close as possible to the critical locus of the loss function, now we start HMC from this stopping point of gradient descent, in three phases:\n",
    "\n",
    "* **3.1 HMC:** run the Markov chain for some time\n",
    "* **3.2 Find a candidate critical point**:\n",
    "* **3.3 Sampling from the local posterior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Run HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 714.0
    },
    "colab_type": "code",
    "id": "8ts-OOCaKs4l",
    "outputId": "1041c7c2-bace-4369-eda0-0fdcde0fd287"
   },
   "outputs": [],
   "source": [
    "# The output chain has the following format: it is a list of eight lists\n",
    "#\n",
    "#  chain = [ c1, ..., c8 ]\n",
    "#\n",
    "# each list corresponds to one of the weight/bias matrices, as in map_initial_state\n",
    "# and for 1 \\le i \\le 8 the list ci contains roughly num_leapfrog_steps*step_size\n",
    "# elements, one for each sample. So if you take [ c1[-1], ..., c8[-1] ] you will\n",
    "# obtain the final tensor, comparable in shape to map_initial_state\n",
    "\n",
    "# NOTE: adaptation only changes step size, not number of leapfrog steps\n",
    "PRE_HMC = False\n",
    "\n",
    "if( PRE_HMC ):\n",
    "  chain, trace, final_kernel_results = run_hmc(\n",
    "    bnn_joint_log_prob,\n",
    "    num_burnin_steps=2000, # was 10000\n",
    "    num_leapfrog_steps=10,\n",
    "    num_adaptation_steps=2000, # was 8000\n",
    "    num_results=15000, # was 30000\n",
    "    step_size=1e-6,\n",
    "    current_state=map_initial_state)\n",
    "\n",
    "  print(\"Acceptance rate:\",\n",
    "        trace.inner_results.is_accepted[-1000:].numpy().mean())\n",
    "\n",
    "  for c in chain:\n",
    "    print(\"ESS/step\", tf.reduce_min(tfp.mcmc.effective_sample_size(c[-1000:]) / 1000).numpy())\n",
    "\n",
    "  plt.figure()\n",
    "  plt.title(\"Chains\")\n",
    "  for i in range(14):\n",
    "    plt.plot(chain[6][:, i, 0])\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.title(\"Step size\")\n",
    "  plt.plot(trace.inner_results.accepted_results.step_size)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.title(\"Distances\")\n",
    "  plot_distances(chain, map_initial_state)\n",
    "  plt.show()\n",
    "  \n",
    "  plot_curves([c[::50] for c in chain])\n",
    "  plt.ylim(-1, 2)\n",
    "  plt.yticks(np.linspace(-1, 2, 16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find our candidate critical point\n",
    "\n",
    "We find a `chain_index` which picks out a position in the chain where the distance was stationary (see the graphs above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( PRE_HMC ):\n",
    "  ball_radius = 2\n",
    "  chain_index = -1 # default to last chain position\n",
    "  num_samples = len(chain[0])\n",
    "\n",
    "  # pos ranges over 0,100,200,...,num_samples\n",
    "  for pos in range(num_samples)[::200]:\n",
    "    acc_rate = trace.inner_results.is_accepted[pos:pos+100].numpy().mean()\n",
    "  \n",
    "    #print(\"Acceptance rate: \" + str(acc_rate) + \" at pos \" + str(pos))\n",
    "    if( acc_rate < 0.01 ):\n",
    "      chain_index = pos\n",
    "\n",
    "  if( chain_index == -1 ):\n",
    "    print(\"Failed to find fixed point\")\n",
    "  else:\n",
    "    print(\"Fixed point located: \" + str(chain_index))\n",
    "  \n",
    "  # Take as the center of our neighborhood the weight value at this approximate\n",
    "  # fixed point of the HMC in the above search\n",
    "  center = [tf.constant(t[chain_index]) for t in chain]\n",
    "else:\n",
    "  center = map_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Estimate the local posterior around our candidate critical point\n",
    "\n",
    "Here we modify the log probability to drop off to zero very quickly outside a ball of radius epsilon of the current weight vector found at the end of the last HMC round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"local\" # global, local\n",
    "local_weight_prior = tfd.Normal(0.0, 1e-5) # mean, std\n",
    "local_bias_prior = tfd.Normal(0.0, 1e-5)\n",
    "\n",
    "def bnn_joint_log_prob_fn_local(center, weight_prior, bias_prior, X, y, *args):\n",
    "    weights_list = args[::2] # every second argument starting with 0\n",
    "    biases_list = args[1::2] # every second argument starting with 1\n",
    "    center_weights = center[::2]\n",
    "    center_biases = center[1::2]\n",
    "  \n",
    "    lp = 0.0\n",
    "    \n",
    "    # prior log-prob\n",
    "    if( prior == \"global\" ):\n",
    "      lp = sum(\n",
    "          [tf.reduce_sum(weight_prior.log_prob(weights)) for weights in weights_list]\n",
    "      )\n",
    "      lp += sum([tf.reduce_sum(bias_prior.log_prob(bias)) for bias in biases_list])\n",
    "    else:\n",
    "      lp = sum(\n",
    "          [tf.reduce_sum(local_weight_prior.log_prob(w-cw)) \n",
    "           for (w,cw) in zip(weights_list,center_weights)]\n",
    "      )\n",
    "      lp += sum([tf.reduce_sum(local_bias_prior.log_prob(b-bw)) \n",
    "            for (b,bw) in zip(biases_list,center_biases)])\n",
    "    \n",
    "    # likelihood of predicted labels\n",
    "    network = build_network(weights_list, biases_list)\n",
    "    labels_dist = network(X.astype(\"float32\"))\n",
    "    lp += tf.reduce_sum(labels_dist.log_prob(y))\n",
    "    \n",
    "    # we also need to add a term that concentrates probability mass\n",
    "    # in a ball around the MAP\n",
    "    #d = 0.0\n",
    "    \n",
    "    #for (x, xprime) in zip(center, args):\n",
    "    #  d += tf.square(tf.norm(x - xprime))\n",
    "    \n",
    "    #d = tf.sqrt(d)\n",
    "    \n",
    "    #lp += tf.cond( d > ball_radius, lambda: -10.0, lambda: 0.0 )\n",
    "      \n",
    "    #print(\"-\")\n",
    "    #tf.print(d)\n",
    "    #tf.print(lp)\n",
    "    \n",
    "    return lp\n",
    "\n",
    "bnn_joint_log_prob_local = partial(\n",
    "    bnn_joint_log_prob_fn_local, center, weight_prior, bias_prior, X_train, y_train[:, 0]\n",
    ")\n",
    "\n",
    "fine_chain, fine_trace, fine_final_kernel_results = run_hmc(\n",
    "    bnn_joint_log_prob_local,\n",
    "    num_burnin_steps=10000,\n",
    "    num_leapfrog_steps=5,\n",
    "    num_adaptation_steps=8000,\n",
    "    num_results=10000,\n",
    "    step_size=1e-6,\n",
    "    current_state=center)\n",
    "\n",
    "print(\"Acceptance rate:\",\n",
    "        fine_trace.inner_results.is_accepted[-1000:].numpy().mean())\n",
    "\n",
    "for c in fine_chain:\n",
    "  print(\"ESS/step\", tf.reduce_min(tfp.mcmc.effective_sample_size(c[-1000:]) / 1000).numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Distances\")\n",
    "plot_distances(fine_chain, center)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "#\n",
    "# Our weight and bias prior are quite strong given the norms involved, \n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Norm of weight vector\")\n",
    "plot_distances(fine_chain, [0.0*x for x in center])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old\n",
    "\n",
    "# NOTE: of course you will not find a long chain of \"zero accept\"\n",
    "# if you adaptation! It will lower the learning rate until acceptances\n",
    "# go back up\n",
    "\n",
    "# TODO: we should just search in the chain of the previous step, not do this again!\n",
    "\n",
    "# Restart chains with a lower learning rate\n",
    "#resume = [chain, trace, final_kernel_results]\n",
    "\n",
    "#hunt_count = 0\n",
    "#max_hunt_count = 30\n",
    "#accept_rate = 1.0\n",
    "\n",
    "#while( accept_rate > 0.1 and hunt_count < max_hunt_count ):\n",
    "#  hunt_count += 1\n",
    "#  print(\"In hunt:\" + str(hunt_count))\n",
    "  \n",
    "#  chain, trace, final_kernel_results = run_hmc(\n",
    "#    bnn_joint_log_prob, # was bnn_joint_log_prob_local,\n",
    "#    num_burnin_steps=0,\n",
    "#    num_leapfrog_steps=10,\n",
    "#    num_adaptation_steps=0,\n",
    "#    num_results=500,\n",
    "#    step_size=1e-5,\n",
    "#    resume=resume)\n",
    "\n",
    "#  accept_rate = trace.inner_results.is_accepted[-500:].numpy().mean()\n",
    "#  print(\"Acceptance rate:\", accept_rate)\n",
    "    \n",
    "#for c in chain:\n",
    "#  print(\"ESS/step\", tf.reduce_min(tfp.mcmc.effective_sample_size(c[-1000:]) / 1000).numpy())\n",
    "\n",
    "#plt.figure()\n",
    "#plt.title(\"Distances\")\n",
    "#plot_distances(chain, map_initial_state)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BNN_HMC (really public).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
