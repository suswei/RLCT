{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook gives good practices that help when applying Vartiational Inference. We are going to use VI to regress a 1D function with a complex noise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "import pyvarinf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "The model used is a two layer MLP. The particularity here is that for every input $x$, the model outputs both a mean prediction $\\mu(x)$ and a standard deviation $\\sigma(x) = \\log(1 + e^{\\rho(x)})$. $\\sigma$ is parameterized this way to avoid negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 32\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\" The model we are going to use \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, n_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_units, n_units),\n",
    "            nn.ReLU())\n",
    "        self.mean = nn.Linear(n_units, 1)\n",
    "        self.rho = nn.Linear(n_units, 1)\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        h = self.model(*inputs)\n",
    "        return self.mean(h), self.rho(h)\n",
    "    \n",
    "model = Model()\n",
    "var_model = pyvarinf.Variationalize(model)\n",
    "\n",
    "if use_cuda:\n",
    "    var_model = var_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating dataset\n",
    "We are attempting to fit a noisy function of the form\n",
    "$$ y = \\cos(3 x) + \\mathcal{N}\\left(0, 1\\right) \\frac{|x|}{2}. $$\n",
    "Samples far from the origin are much noisier than samples close to it. Using the usual mean square error with a single output MLP is ill advised here, as it assumes a gaussian noise model with fixed variance.\n",
    "\n",
    "Note that when you use the traditional mean square error, you usually assume that your probabilitic model is\n",
    "$$ p(y\\mid x) = \\mathcal{N}\\left(y\\mid \\mu=0, \\sigma^2=1\\right).$$\n",
    "When using VI, to have a proper balance between the prior loss, which is a regularization term, and the Maximum likelihood term, you must select a $\\sigma$ that makes sense for your data. If your data lie between $0$ and $0.01$, a model fitting the data with precision $\\sigma=1$ is meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    \"\"\" Generate n_samples regression datapoints \"\"\"\n",
    "    x = np.random.normal(size=(n_samples, 1))\n",
    "    y = np.cos(x * 3) + np.random.normal(size=(n_samples, 1)) * np.abs(x) / 2\n",
    "    return x, y\n",
    "\n",
    "def batch_iterator(x, y):\n",
    "    \"\"\" Provides an iterator given data and labels \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    def _iterator(batch_size):\n",
    "        sample_indices = np.random.randint(0, high=n_samples, size=batch_size)\n",
    "        return x[sample_indices], y[sample_indices]\n",
    "    return _iterator\n",
    "    \n",
    "n_train_data = 5000\n",
    "n_test_data = 100\n",
    "train_x, train_y = generate_data(n_train_data)\n",
    "test_x, test_y = generate_data(n_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize outputs\n",
    "VI depends more on the prior/initialization than Maximum Likelihood Training. With Xavier initialization or He initialization, the order of magnitude of the outputs of the neural network is around 1 if the inputs are normalized. Setting the targets to remain close to that order of magnitude helps a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y / np.std(train_y)\n",
    "test_y = test_y / np.std(test_y)\n",
    "\n",
    "train_iterator = batch_iterator(train_x, train_y)\n",
    "plt.scatter(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "n_samples = 4\n",
    "def init_plot():\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.set_xlim(-5, 5)\n",
    "    ax1.set_ylim(-10, 10)\n",
    "    ax1.set_xlabel(\"x\", fontsize=20)\n",
    "    ax1.set_ylabel(\"y\", fontsize=20)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.set_xlim(0, n_epochs)\n",
    "    ax2.set_ylim(-1.25, 1.25)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=20)\n",
    "    ax2.set_ylabel(\"Variational bound estimate\", fontsize=20)\n",
    "    lines = [ax1.plot(test_x, test_y, ls='', marker='o', alpha=0.7)]\n",
    "    for _ in range(n_samples):\n",
    "        lines += ax1.plot([0], [0], ls='', marker='o', alpha=0.3)\n",
    "    loss_line, = ax2.plot([0.], [0.], alpha=0.7)\n",
    "    prior_loss_line, = ax2.plot([0.], [0.], alpha=0.7)\n",
    "    mle_loss_line, = ax2.plot([0.], [0.], alpha=0.7)\n",
    "    ax2.legend([loss_line, prior_loss_line, mle_loss_line],\n",
    "               [\"Loss\", \"Prior loss\", \"MLE loss\"])\n",
    "    return lines, loss_line, prior_loss_line, mle_loss_line\n",
    "    \n",
    "def plot(lines, loss_line, prior_loss_line, mle_loss_line,\n",
    "         epoch, loss, prior_loss, mle_loss):\n",
    "    \"\"\" Plotting utility \"\"\"\n",
    "    # sample n models from the posterior\n",
    "    if lines is None:\n",
    "        lines, loss_line, prior_loss_line, mle_loss_line = init_plot()\n",
    "    l0, lines = lines[0], lines[1:]\n",
    "    n_samples = 10\n",
    "    nets = [pyvarinf.Sample(var_model) for _ in range(n_samples)]\n",
    "    for net in nets:\n",
    "        net.draw()\n",
    "    x_space = np.random.normal(size=(1000,))\n",
    "    y_spaces = []\n",
    "    for net in nets:\n",
    "        inputs = Variable(torch.Tensor(np.expand_dims(x_space, 1)))\n",
    "        noise = Variable(torch.randn(1000, 1))\n",
    "        if use_cuda:\n",
    "            noise = noise.cuda()\n",
    "            inputs = inputs.cuda()\n",
    "        mean, rho = net(inputs)\n",
    "        outputs = mean + torch.log(1 + torch.exp(rho)) * noise\n",
    "        y_spaces += [outputs.squeeze().data.cpu().numpy()]\n",
    "    for l, y_space in zip(lines, y_spaces):\n",
    "        l.set_data(x_space, y_space)\n",
    "    lines = [l0] + lines\n",
    "    \n",
    "    epochs = loss_line.get_xdata()\n",
    "    losses = loss_line.get_ydata()\n",
    "    prior_losses = prior_loss_line.get_ydata()\n",
    "    mle_losses = mle_loss_line.get_ydata()\n",
    "    epochs = list(epochs) + [epoch]\n",
    "    losses = list(losses) + [loss]\n",
    "    prior_losses = list(prior_losses) + [prior_loss]\n",
    "    mle_losses = list(mle_losses) + [mle_loss]\n",
    "    loss_line.set_data(epochs, losses)\n",
    "    prior_loss_line.set_data(epochs, prior_losses)\n",
    "    mle_loss_line.set_data(epochs, mle_losses)\n",
    "    return lines, loss_line, prior_loss_line, mle_loss_line\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define log likelihood loss\n",
    "We are learning both the mean and the variance of the noisy function we fit. The probabilistic model represented by our model is $p(y|x) = \\mathcal{N}\\left(y \\mid \\mu(x), \\log(1 + e^{\\rho(x)})^2\\right)$. We must define the associated log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_fit_loss(data, mean, rho):\n",
    "    \"\"\" Compute log likelihood of data, assuming \n",
    "    a N(mean, log(1 + e^rho)) model.\n",
    "    \"\"\"\n",
    "    sigma = torch.log(1 + torch.exp(rho))\n",
    "    return torch.mean((mean - data) ** 2 / (2 * sigma ** 2)) + torch.mean(torch.log(sigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The model is trained as any other pytorch model, and we simply add the prior loss as a regularizer. The more data you have, the less you should regularize your model, which explains in a handwavy way why we divide the prior loss by the number of training examples. Have a look at the README for a more formal explaination.\n",
    "\n",
    "VI is quite slow, due to the added randomness. You will still obtain a good fit in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations_per_epochs = 100\n",
    "batch_size = 256\n",
    "mle_samples = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(var_model.parameters(), lr=5e-3)\n",
    "\n",
    "lines = None\n",
    "loss_line, prior_loss_line, mle_loss_line = None, None, None\n",
    "for e in range(n_epochs):\n",
    "    for i in range(n_iterations_per_epochs):\n",
    "        batch_x, batch_y = train_iterator(batch_size)\n",
    "        batch_x, batch_y = [Variable(torch.Tensor(arr)) for arr in [batch_x, batch_y]]\n",
    "        if use_cuda:\n",
    "            batch_x, batch_y = [arr.cuda() for arr in [batch_x, batch_y]]\n",
    "        mle_loss = 0\n",
    "        for _ in range(mle_samples):\n",
    "            mean, rho = var_model(batch_x)\n",
    "            mle_loss += gaussian_fit_loss(batch_y, mean, rho)\n",
    "        mle_loss = mle_loss / mle_samples\n",
    "        prior_loss = var_model.prior_loss() / n_train_data\n",
    "        loss = mle_loss + prior_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    display.clear_output(wait=True)\n",
    "    lines, loss_line, prior_loss_line, mle_loss_line = plot(\n",
    "        lines, loss_line, prior_loss_line, mle_loss_line,\n",
    "        e, loss.data[0], prior_loss.data[0], mle_loss.data[0])\n",
    "    display.display(pl.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "One could think that the use of our \"complex\" noise model is unnecessary here: VI introduces noise in the network, and this randomness could deal with the randomness of the function we have to fit. This is not the case. \n",
    "\n",
    "If we apply VI with a simple MSE loss on this task, we will obtain very good fits of the mean of the noisy function, but the model will ignore the function noise. \n",
    "\n",
    "To verify this fact, take the infinite data case. In this case, the network only suffers the MLE loss, and not the prior loss. The deterministic function that minimizes the mean square error is the mean of the noisy function. Thus, the mixture model that minimizes the mean square error on average is a mixture that only weights parameters that represent the mean of the noisy function (if you weight any other function, you will obtain a better average performance by reducing the weight on this function and readding an equivalent weight to the mean)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
