{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "The goal of this notebook is to examine RLCT estimation in 2D for three toy synthetic datasets. We will compare the performance of implicit variational inference versus expliciit variational inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the parameters which feed into our training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from main import *\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    syntheticsamplesize = 500\n",
    "    batchsize = 100\n",
    "    w_dim = 2\n",
    "    dpower = None\n",
    "    posterior_viz = True\n",
    "    sanity_check = True\n",
    "\n",
    "    epochs = 200\n",
    "    prior = 'gaussian'\n",
    "    pretrainDepochs = 100\n",
    "    trainDepochs = 50\n",
    "    n_hidden_D = 128\n",
    "    num_hidden_layers_D = 1\n",
    "    n_hidden_G = 128\n",
    "    num_hidden_layers_G = 1\n",
    "\n",
    "    lr_primal = 1e-3\n",
    "    lr_dual = 1e-3\n",
    "    lr = 1e-2\n",
    "\n",
    "    beta_auto_liberal = False\n",
    "    beta_auto_conservative = False\n",
    "    beta_auto_oracle = False\n",
    "    betasbegin = 0.1\n",
    "    betasend = 1.5\n",
    "    betalogscale = True\n",
    "    numbetas = 5\n",
    "\n",
    "    elasticnet_alpha = 0.5\n",
    "    R = 200\n",
    "\n",
    "    cuda = False\n",
    "\n",
    "    log_interval = 50\n",
    "    notebook = True\n",
    "    \n",
    "args = Args()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function draws a dataset, trains both explicit and implicit variational inference. Results are logged as posterior graphs and RLCT least squares plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    # draw new training-testing split\n",
    "    train_loader, valid_loader, test_loader = get_dataset_by_id(args, kwargs)\n",
    "\n",
    "    # get a grid of inverse temperatures [beta_1/log n, \\ldots, beta_k/log n]\n",
    "    set_betas(args)\n",
    "\n",
    "    mc = 1\n",
    "    saveimgpath = None\n",
    "    nll_betas_explicit = np.empty(0)\n",
    "    nll_betas_implicit = np.empty(0)\n",
    "\n",
    "    for beta_index in range(args.betas.shape[0]):\n",
    "\n",
    "        # train explicit variational inference\n",
    "        print('Begin training EVI')\n",
    "        var_model = train_explicitVI(train_loader, valid_loader, args, mc, beta_index, True, saveimgpath)\n",
    "        print('Finished training EVI')\n",
    "        nllw_array_explicit = approxinf_nll_explicit(train_loader, var_model, args)\n",
    "        # record E nL_n(w)\n",
    "        nll_betas_explicit = np.append(nll_betas_explicit, sum(nllw_array_explicit)/len(nllw_array_explicit))\n",
    "\n",
    "        # visualize EVI\n",
    "        args.VItype = 'explicit'\n",
    "        sampled_weights = sample_EVI(var_model, args)\n",
    "        posterior_viz(train_loader, sampled_weights, args, beta_index, saveimgpath)\n",
    "\n",
    "        # train implicit variational inference\n",
    "        args.epsilon_dim = args.w_dim\n",
    "        args.epsilon_mc = args.batchsize\n",
    "        args.VItype = 'implicit'\n",
    "        print('Begin training IVI')\n",
    "        G = train_implicitVI(train_loader, valid_loader, args, mc, beta_index, saveimgpath)\n",
    "        print('Finished training IVI')\n",
    "        nllw_array_implicit = approxinf_nll_implicit(train_loader, G, args)\n",
    "        nll_betas_implicit = np.append(nll_betas_implicit, sum(nllw_array_implicit)/len(nllw_array_implicit))\n",
    "\n",
    "        # visualize IVI\n",
    "        with torch.no_grad():\n",
    "            eps = torch.randn(100, args.epsilon_dim)\n",
    "            sampled_weights = G(eps)\n",
    "            posterior_viz(train_loader, sampled_weights, args, beta_index, saveimgpath)\n",
    "    \n",
    "    return nll_betas_explicit, nll_betas_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "The binary target is modeled as $y = bernoulli(p)$, where $$p = \\exp(w^T x + b)/(1+\\exp(w^T x + b))$$ with $x$ is standard Gaussian. \n",
    "We set the true parameters to $w_0 = (0.5, 1)$ and $b = 0.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'logistic_synthetic'\n",
    "args.network = 'logistic'\n",
    "args.bias = False\n",
    "args.input_dim = args.w_dim\n",
    "args.output_dim = 1\n",
    "\n",
    "# Let's generate some data according to this model\n",
    "args.w_0 = torch.Tensor([[0.5], [1]])\n",
    "args.b = torch.tensor([0.0])\n",
    "X = torch.randn(2 * args.syntheticsamplesize, args.input_dim)\n",
    "affine = torch.mm(X, args.w_0) + args.b\n",
    "m = torch.distributions.bernoulli.Bernoulli(torch.sigmoid(affine))\n",
    "y = m.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(affine.squeeze(dim=1).detach().numpy(), y.detach().numpy(), '.g')\n",
    "plt.plot(affine.squeeze(dim=1).detach().numpy(), torch.sigmoid(affine).detach().numpy(), '.r')\n",
    "plt.xlabel('w^t x + b')\n",
    "plt.ylabel('probs and binary response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next let's train explicit and implicit variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_betas_explicit, nll_betas_implicit = train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the variational infernece methods can estimate the RLCT, \n",
    "we plot $1/\\beta$ versus the VI estimate of $E_w^\\beta nL_n(w)$. \n",
    "The true RLCT should be the slope of this scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Explicit VI: RLCT estimate')\n",
    "lsfit_lambda(nll_betas_explicit, args, saveimgpath=None)\n",
    "print('Implicit VI: RLCT estimate')\n",
    "lsfit_lambda(nll_betas_implicit, args, saveimgpath=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh regression\n",
    "The univariate continuous target is modeled as $y = N(f(x,a,b),1)$,\n",
    "where $$f(x,a,b) = \\sum_{h=1}^H a_h \\tanh(b_hx)$$ with $x \\tilde uniform(0,1)$.\n",
    "We will consider a two dimensional model where $H=1$ and $(a,b)=(0,0)$.\n",
    "Note that this is a singular model where $\\{a,b: K(a,b) = 0\\} = \\{a,b \\in \\mathbb R: ab = 0\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args.dataset = 'tanh_synthetic'\n",
    "args.network = 'tanh'\n",
    "args.bias = False\n",
    "args.H = 1\n",
    "args.a_params = torch.zeros([1, args.H], dtype=torch.float32)\n",
    "args.b_params = torch.zeros([args.H, 1], dtype=torch.float32)\n",
    "args.input_dim = 1\n",
    "args.output_dim = 1\n",
    "\n",
    "m = Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "X = m.sample(torch.Size([2 * args.syntheticsamplesize]))\n",
    "mean = torch.matmul(torch.tanh(torch.matmul(X, args.a_params)), args.b_params)\n",
    "y_rv = Normal(mean, 1)\n",
    "y = y_rv.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X.detach().numpy(), y.detach().numpy(), '.g')\n",
    "plt.title('synthetic tanh regression data: x versus y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's train explicit and implicit variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_betas_explicit, nll_betas_implicit = train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the variational infernece methods can estimate the RLCT,\n",
    "we plot $1/\\beta$ versus the VI estimate of $E_w^\\beta nL_n(w)$.\n",
    "The true RLCT should be the slope of this scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Explicit VI: RLCT estimate')\n",
    "lsfit_lambda(nll_betas_explicit, args, saveimgpath=None)\n",
    "print('Implicit VI: RLCT estimate')\n",
    "lsfit_lambda(nll_betas_implicit, args, saveimgpath=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced rank regression\n",
    "The univariate continuous target is modeled as $y = N(f(x,A,B),1)$,\n",
    "where $$f(x,A,B) = BAx$$ with $x$ standard Gaussian.\n",
    "Considering a two dimensional model where we set the two matrices each to the scalar $1$.\n",
    "Note that this is a singular model where we have $\\{a,b: K(a,b) = 0\\} = \\{a,b \\in \\mathbb R: ab = 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args.dataset = 'reducedrank_synthetic'\n",
    "args.network = 'reducedrank'\n",
    "args.bias = False\n",
    "args.a_params = torch.Tensor([1.0]).reshape(1, 1)\n",
    "args.b_params = torch.Tensor([1.0]).reshape(1, 1)\n",
    "args.input_dim = 1\n",
    "args.output_dim = 1\n",
    "args.H = 1\n",
    "\n",
    "# Let's generate some data according to this model\n",
    "m = MultivariateNormal(torch.zeros(args.input_dim), torch.eye(\n",
    "    args.input_dim))  # the input_dim=output_dim + 3, output_dim = H (the number of hidden units)\n",
    "X = m.sample(torch.Size([2 * args.syntheticsamplesize]))\n",
    "mean = torch.matmul(torch.matmul(X, args.a_params), args.b_params)\n",
    "y_rv = MultivariateNormal(mean, torch.eye(args.output_dim))\n",
    "y = y_rv.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X.detach().numpy(), y.detach().numpy(), '.g')\n",
    "plt.title('synthetic reduced rank regression data: x versus y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's train explicit and implicit variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_betas_explicit, nll_betas_implicit = train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the variational infernece methods can estimate the RLCT,\n",
    "we plot $1/\\beta$ versus the VI estimate of $E_w^\\beta nL_n(w)$.\n",
    "The true RLCT should be the slope of this scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Explicit VI: RLCT estimate')\n",
    "evi_robust, evi_ols = lsfit_lambda(nll_betas_explicit, args, saveimgpath=None)\n",
    "print('Implicit VI: RLCT estimate')\n",
    "ivi_robust, ivi_ols = lsfit_lambda(nll_betas_implicit, args, saveimgpath=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}