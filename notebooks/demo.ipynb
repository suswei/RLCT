{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this notebook is to examine RLCT estimation in 2D for three toy synthetic datasets. We will compare the performance of implicit variational inference versus expliciit variational inference. blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the parameters which feed into our main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from main import *\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    syntheticsamplesize = 500\n",
    "    batchsize = 100\n",
    "    w_dim = 2\n",
    "    dpower = None\n",
    "    posterior_viz = True\n",
    "\n",
    "    epochs = 200\n",
    "    prior = 'gaussian'\n",
    "    pretrainDepochs = 100\n",
    "    trainDepochs = 50\n",
    "    n_hidden_D = 128\n",
    "    num_hidden_layers_D = 1\n",
    "    n_hidden_G = 128\n",
    "    num_hidden_layers_G = 1\n",
    "\n",
    "    lr_primal = 1e-3\n",
    "    lr_dual = 1e-3\n",
    "    lr = 1e-2\n",
    "\n",
    "    elasticnet_alpha = 0.5\n",
    "\n",
    "    beta_auto_liberal = False\n",
    "    beta_auto_conservative = False\n",
    "    beta_auto_oracle = False\n",
    "    betasbegin = 0.1\n",
    "    betasend = 1.9\n",
    "    betalogscale = True\n",
    "    numbetas = 3\n",
    "\n",
    "    R = 200\n",
    "\n",
    "    cuda = False\n",
    "\n",
    "    log_interval = 100\n",
    "    \n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function draws a dataset, trains both explicit and implicit variational inference. Results are logged as posterior graphs and RLCT least squares plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # draw new training-testing split\n",
    "    train_loader, valid_loader, test_loader = get_dataset_by_id(args, kwargs)\n",
    "\n",
    "    # get a grid of inverse temperatures [beta_1/log n, \\ldots, beta_k/log n]\n",
    "    set_betas(args)\n",
    "\n",
    "    mc = 1\n",
    "    saveimgpath = None\n",
    "    nll_betas_explicit = np.empty(0)\n",
    "    nll_betas_implicit = np.empty(0)\n",
    "\n",
    "    for beta_index in range(args.betas.shape[0]):\n",
    "\n",
    "        # train explicit variational inference\n",
    "        var_model = train_explicitVI(train_loader, valid_loader, args, mc, beta_index, True, saveimgpath)\n",
    "        nllw_array_explicit = approxinf_nll_explicit(train_loader, var_model, args)\n",
    "        # record E nL_n(w)\n",
    "        nll_betas_explicit = np.append(nll_betas_explicit, nllw_array_explicit.mean())\n",
    "\n",
    "        # visualize EVI\n",
    "        args.VItype = 'explicit'\n",
    "        sampled_weights = sample_EVI(var_model, args)\n",
    "        posterior_viz(train_loader, sampled_weights, args, beta_index, saveimgpath)\n",
    "\n",
    "        # train implicit variational inference\n",
    "        args.epsilon_dim = args.w_dim\n",
    "        args.epsilon_mc = args.batchsize\n",
    "        args.VItype = 'implicit'\n",
    "        G = train_implicitVI(train_loader, valid_loader, args, mc, beta_index, saveimgpath)\n",
    "        nllw_array_implicit = approxinf_nll_implicit(train_loader, G, args)\n",
    "        nll_betas_implicit = np.append(nll_betas_implicit, nllw_array_implicit.mean())\n",
    "\n",
    "        # visualize IVI\n",
    "        with torch.no_grad():\n",
    "            eps = torch.randn(100, args.epsilon_dim)\n",
    "            sampled_weights = G(eps)\n",
    "            posterior_viz(train_loader, sampled_weights, args, beta_index, saveimgpath)\n",
    "\n",
    "\n",
    "    # should observe a straight line below\n",
    "    lsfit_lambda(nll_betas_explicit, args, saveimgpath)\n",
    "    lsfit_lambda(nll_betas_implicit, args, saveimgpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The target is generated as $y = bernoulli(p)$, where $p = 1/(1+e^-(w^T x + b)$ with $x \\sim n(0,1)$\n",
    "We set the true parameters to w = [0.5, 1] and b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'logistic_synthetic'\n",
    "args.network = 'logistic'\n",
    "args.bias = False\n",
    "args.input_dim = args.w_dim\n",
    "args.output_dim = 1\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "\n",
    "# Let's generate some data according to this model\n",
    "args.w_0 = torch.Tensor([[0.5], [1]])\n",
    "args.b = torch.tensor([0.0])\n",
    "X = torch.randn(2 * args.syntheticsamplesize, args.input_dim)\n",
    "affine = torch.mm(X, args.w_0) + args.b\n",
    "m = torch.distributions.bernoulli.Bernoulli(torch.sigmoid(affine))\n",
    "y = m.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(affine.squeeze(dim=1).detach().numpy(), y.detach().numpy(), '.g')\n",
    "plt.plot(affine.squeeze(dim=1).detach().numpy(), torch.sigmoid(affine).detach().numpy(), '.r')\n",
    "plt.title('synthetic logistic regression data: w^T x + b versus probabilities and Bernoulli(p)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
